{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"BRAF The braf module implements the Biased Random Forest ( link ). The only dependencies are python3.7+ , numpy , pandas , and matplotlib . To install, just run: $ python setup.py install The script run/train_and_evaluate.py will train on a supplied dataset. Plots, model artifacts, and results will be saved to a supplied output directory. Some results are also printed to stdout. Example usage: $ ./train_and_eval.py --dataset ../../diabetes.csv --output_path ../../results/ ROC and PRC curves placed in ../../results/ Cross-validation set results: auroc = 0 .836 auprc = 0 .686 precision = 0 .667 recall = 0 .590 Test set results: auroc = 0 .814 auprc = 0 .682 precision = 0 .628 recall = 0 .584 Also provided is a helper script run/hyperparam_opt.py , which will train and evaluate over a grid of hyperparameter options. This was used to set defaults in train_and_evaluate.py , aside from the provided defaults (K=10, s=100, p=0.5).","title":"Home"},{"location":"#braf","text":"The braf module implements the Biased Random Forest ( link ). The only dependencies are python3.7+ , numpy , pandas , and matplotlib . To install, just run: $ python setup.py install The script run/train_and_evaluate.py will train on a supplied dataset. Plots, model artifacts, and results will be saved to a supplied output directory. Some results are also printed to stdout. Example usage: $ ./train_and_eval.py --dataset ../../diabetes.csv --output_path ../../results/ ROC and PRC curves placed in ../../results/ Cross-validation set results: auroc = 0 .836 auprc = 0 .686 precision = 0 .667 recall = 0 .590 Test set results: auroc = 0 .814 auprc = 0 .682 precision = 0 .628 recall = 0 .584 Also provided is a helper script run/hyperparam_opt.py , which will train and evaluate over a grid of hyperparameter options. This was used to set defaults in train_and_evaluate.py , aside from the provided defaults (K=10, s=100, p=0.5).","title":"BRAF"},{"location":"reference/braf/","text":"Module braf View Source from . import braf , utils , plot , knn , forest Sub-modules braf.braf braf.forest braf.knn braf.plot braf.utils","title":"Index"},{"location":"reference/braf/#module-braf","text":"View Source from . import braf , utils , plot , knn , forest","title":"Module braf"},{"location":"reference/braf/#sub-modules","text":"braf.braf braf.forest braf.knn braf.plot braf.utils","title":"Sub-modules"},{"location":"reference/braf/braf/","text":"Module braf.braf View Source from .knn import KNN from .forest import RandomForest import numpy as np class BRAF : ''' Biased Random Forest. Parameters: - K (int): the number of nearest neighbors - s (int): total number of trees to train - p (float): fraction of s trees to fit on the critical subset - minority_label (int): the label corresponding to the minority class ''' def __init__ ( self , K , s , p , minority_label , * args , ** kwargs ): self . knn = KNN ( K ) self . rf1 = RandomForest ( int ( s * ( 1 - p )), * args , ** kwargs ) self . rf2 = RandomForest ( int ( s * p ), * args , ** kwargs ) self . p = p self . min_label = minority_label def fit ( self , X , y ): ''' Fit BRAF Parameters: - X (ndarray): features - y (ndaray): labels ''' critical_idcs = self . knn . get_neighbors ( X [ y == self . min_label ], X [ y != self . min_label ]) # critical_idcs contains duplicates, so flatten and remove critical_idcs = np . array ( list ( set ( critical_idcs . reshape ( - 1 )))) # fit on the whole dataset self . rf1 . fit ( X , y ) # fit on the critical area self . rf2 . fit ( X [ critical_idcs , :], y [ critical_idcs ]) def predict ( self , X ): ''' Run inference Parameters - X (ndarray): features ''' # return a weighted sum of the predictions yhat1 = self . rf1 . predict ( X ) yhat2 = self . rf2 . predict ( X ) return (( 1 - self . p ) * yhat1 ) + ( self . p * yhat2 ) def __str__ ( self ): s = '=============== RF1 =================' s += str ( self . rf1 ) s += ' \\n\\n =============== RF2 =================' s += str ( self . rf2 ) Classes BRAF class BRAF ( K , s , p , minority_label , * args , ** kwargs ) Biased Random Forest. Parameters: K (int): the number of nearest neighbors s (int): total number of trees to train p (float): fraction of s trees to fit on the critical subset minority_label (int): the label corresponding to the minority class View Source class BRAF : ''' Biased Random Forest. Parameters: - K (int): the number of nearest neighbors - s (int): total number of trees to train - p (float): fraction of s trees to fit on the critical subset - minority_label (int): the label corresponding to the minority class ''' def __init__ ( self , K , s , p , minority_label , * args , ** kwargs ) : self . knn = KNN ( K ) self . rf1 = RandomForest ( int ( s * ( 1 - p )), * args , ** kwargs ) self . rf2 = RandomForest ( int ( s * p ), * args , ** kwargs ) self . p = p self . min_label = minority_label def fit ( self , X , y ) : ''' Fit BRAF Parameters: - X (ndarray): features - y (ndaray): labels ''' critical_idcs = self . knn . get_neighbors ( X [ y==self.min_label ] , X [ y!=self.min_label ] ) # critical_idcs contains duplicates , so flatten and remove critical_idcs = np . array ( list ( set ( critical_idcs . reshape ( - 1 )))) # fit on the whole dataset self . rf1 . fit ( X , y ) # fit on the critical area self . rf2 . fit ( X [ critical_idcs, : ] , y [ critical_idcs ] ) def predict ( self , X ) : ''' Run inference Parameters - X (ndarray): features ''' # return a weighted sum of the predictions yhat1 = self . rf1 . predict ( X ) yhat2 = self . rf2 . predict ( X ) return (( 1 - self . p ) * yhat1 ) + ( self . p * yhat2 ) def __str__ ( self ) : s = '=============== RF1 =================' s += str ( self . rf1 ) s += '\\n\\n=============== RF2 =================' s += str ( self . rf2 ) Methods fit def fit ( self , X , y ) Fit BRAF Parameters: X (ndarray): features y (ndaray): labels View Source def fit ( self , X , y ) : ''' Fit BRAF Parameters: - X (ndarray): features - y (ndaray): labels ''' critical_idcs = self . knn . get_neighbors ( X [ y==self.min_label ] , X [ y!=self.min_label ] ) # critical_idcs contains duplicates , so flatten and remove critical_idcs = np . array ( list ( set ( critical_idcs . reshape ( - 1 )))) # fit on the whole dataset self . rf1 . fit ( X , y ) # fit on the critical area self . rf2 . fit ( X [ critical_idcs, : ] , y [ critical_idcs ] ) predict def predict ( self , X ) Run inference Parameters X (ndarray): features View Source def predict ( self , X ): ''' Run inference Parameters - X (ndarray): features ''' # return a weighted sum of the predictions yhat1 = self . rf1 . predict ( X ) yhat2 = self . rf2 . predict ( X ) return (( 1 - self . p ) * yhat1 ) + ( self . p * yhat2 )","title":"Braf"},{"location":"reference/braf/braf/#module-brafbraf","text":"View Source from .knn import KNN from .forest import RandomForest import numpy as np class BRAF : ''' Biased Random Forest. Parameters: - K (int): the number of nearest neighbors - s (int): total number of trees to train - p (float): fraction of s trees to fit on the critical subset - minority_label (int): the label corresponding to the minority class ''' def __init__ ( self , K , s , p , minority_label , * args , ** kwargs ): self . knn = KNN ( K ) self . rf1 = RandomForest ( int ( s * ( 1 - p )), * args , ** kwargs ) self . rf2 = RandomForest ( int ( s * p ), * args , ** kwargs ) self . p = p self . min_label = minority_label def fit ( self , X , y ): ''' Fit BRAF Parameters: - X (ndarray): features - y (ndaray): labels ''' critical_idcs = self . knn . get_neighbors ( X [ y == self . min_label ], X [ y != self . min_label ]) # critical_idcs contains duplicates, so flatten and remove critical_idcs = np . array ( list ( set ( critical_idcs . reshape ( - 1 )))) # fit on the whole dataset self . rf1 . fit ( X , y ) # fit on the critical area self . rf2 . fit ( X [ critical_idcs , :], y [ critical_idcs ]) def predict ( self , X ): ''' Run inference Parameters - X (ndarray): features ''' # return a weighted sum of the predictions yhat1 = self . rf1 . predict ( X ) yhat2 = self . rf2 . predict ( X ) return (( 1 - self . p ) * yhat1 ) + ( self . p * yhat2 ) def __str__ ( self ): s = '=============== RF1 =================' s += str ( self . rf1 ) s += ' \\n\\n =============== RF2 =================' s += str ( self . rf2 )","title":"Module braf.braf"},{"location":"reference/braf/braf/#classes","text":"","title":"Classes"},{"location":"reference/braf/braf/#braf","text":"class BRAF ( K , s , p , minority_label , * args , ** kwargs ) Biased Random Forest. Parameters: K (int): the number of nearest neighbors s (int): total number of trees to train p (float): fraction of s trees to fit on the critical subset minority_label (int): the label corresponding to the minority class View Source class BRAF : ''' Biased Random Forest. Parameters: - K (int): the number of nearest neighbors - s (int): total number of trees to train - p (float): fraction of s trees to fit on the critical subset - minority_label (int): the label corresponding to the minority class ''' def __init__ ( self , K , s , p , minority_label , * args , ** kwargs ) : self . knn = KNN ( K ) self . rf1 = RandomForest ( int ( s * ( 1 - p )), * args , ** kwargs ) self . rf2 = RandomForest ( int ( s * p ), * args , ** kwargs ) self . p = p self . min_label = minority_label def fit ( self , X , y ) : ''' Fit BRAF Parameters: - X (ndarray): features - y (ndaray): labels ''' critical_idcs = self . knn . get_neighbors ( X [ y==self.min_label ] , X [ y!=self.min_label ] ) # critical_idcs contains duplicates , so flatten and remove critical_idcs = np . array ( list ( set ( critical_idcs . reshape ( - 1 )))) # fit on the whole dataset self . rf1 . fit ( X , y ) # fit on the critical area self . rf2 . fit ( X [ critical_idcs, : ] , y [ critical_idcs ] ) def predict ( self , X ) : ''' Run inference Parameters - X (ndarray): features ''' # return a weighted sum of the predictions yhat1 = self . rf1 . predict ( X ) yhat2 = self . rf2 . predict ( X ) return (( 1 - self . p ) * yhat1 ) + ( self . p * yhat2 ) def __str__ ( self ) : s = '=============== RF1 =================' s += str ( self . rf1 ) s += '\\n\\n=============== RF2 =================' s += str ( self . rf2 )","title":"BRAF"},{"location":"reference/braf/braf/#methods","text":"","title":"Methods"},{"location":"reference/braf/braf/#fit","text":"def fit ( self , X , y ) Fit BRAF Parameters: X (ndarray): features y (ndaray): labels View Source def fit ( self , X , y ) : ''' Fit BRAF Parameters: - X (ndarray): features - y (ndaray): labels ''' critical_idcs = self . knn . get_neighbors ( X [ y==self.min_label ] , X [ y!=self.min_label ] ) # critical_idcs contains duplicates , so flatten and remove critical_idcs = np . array ( list ( set ( critical_idcs . reshape ( - 1 )))) # fit on the whole dataset self . rf1 . fit ( X , y ) # fit on the critical area self . rf2 . fit ( X [ critical_idcs, : ] , y [ critical_idcs ] )","title":"fit"},{"location":"reference/braf/braf/#predict","text":"def predict ( self , X ) Run inference Parameters X (ndarray): features View Source def predict ( self , X ): ''' Run inference Parameters - X (ndarray): features ''' # return a weighted sum of the predictions yhat1 = self . rf1 . predict ( X ) yhat2 = self . rf2 . predict ( X ) return (( 1 - self . p ) * yhat1 ) + ( self . p * yhat2 )","title":"predict"},{"location":"reference/braf/forest/","text":"Module braf.forest View Source import numpy as np import tqdm from functools import partial EPS = 1e-12 # put this in top-level scope so it can be pickled def _return_const ( * args , ** kwargs ): return kwargs [ 'val' ] class BinaryDecisionNode : def __init__ ( self , X , y , node_depth = 0 , n_search_pts = 100 , max_features = None ): ''' Binary decision node. On construction, the BinaryDecisionNode fits itself to the provided data. If node_depth > 0, it will create and fit two nodes on the results of this node Parameters: - X (ndarray): features - y (ndarray): binary labels - node_depth (int): number of nodes to descend - n_search_pts (int): number of points in the grid used to search for an optimal decision threshold - max_features (int): maximum number of features to optimize over at each node ''' # we assume data is in the form (samples, features) # and labels are a 1D binary vector assert len ( X . shape ) == 2 assert len ( y . shape ) == 1 self . decision_boundary = None self . decision_feature = None # randomly sample at most max_features for fitting if max_features is None : features = np . arange ( X . shape [ 1 ]) else : features = np . random . permutation ( X . shape [ 1 ])[: max_features ] # gini index of the data fed into this node gini0 = self . gini_index ( y ) best_delta = 0 for i_ft in features : # iterate over the possible features, find the best # boundary and the corresponding gini index. pick the # choice of i_ft and boundary that maximizes the drop # from gini0 (new_delta) x = X [:, i_ft ] boundary , new_delta = self . max_delta_gini ( x = x , y = y , lo = np . min ( x ), hi = np . max ( x ), n_search_pts = n_search_pts , gini0 = gini0 ) if new_delta > best_delta : best_delta = new_delta self . decision_feature = i_ft self . decision_boundary = boundary leaf = True # whether this node is a leaf in the tree if self . decision_boundary is not None : # we found a valid decision boundary # run inference on this node to split the dataset. mask = self ( X , descend = False ) # save the mean values of the left and right splits. # not strictly needed if this is not a leaf, but it's # useful for debugging. could also be useful if we ever # prune the tree. self . right_val = y [ mask ] . sum () / ( y [ mask ] . shape [ 0 ] + EPS ) self . left_val = y [ ~ mask ] . sum () / ( y [ ~ mask ] . shape [ 0 ] + EPS ) if node_depth > 0 : # if we haven't reached the max node depth, fit left # and right nodes: self . left = BinaryDecisionNode ( X [ ~ mask ], y [ ~ mask ], node_depth = node_depth - 1 , n_search_pts = n_search_pts ) self . right = BinaryDecisionNode ( X [ mask ], y [ mask ], node_depth = node_depth - 1 , n_search_pts = n_search_pts ) leaf = False # has children -> not a leaf else : # no valid decision boundary, so left = right. use the mean # of the current dataset as the prediction for this node self . left_val = self . right_val = np . mean ( y ) if leaf : # if no children, make the 'children' the mean value in each category self . right = partial ( _return_const , val = self . right_val ) self . left = partial ( _return_const , val = self . left_val ) def __call__ ( self , X , y = None , descend = True ): ''' infer the decision node on a dataset. if descend=true, we traverse the children until we hit leaves. otherwise, just return a boolean array of the decision at this node Parameters: - X (ndarray): features - y (ndarray): labels, optional - descend (boolean): should we descend down into children nodes or not Returns: - ndarray: if descend, we guarantee to end at a leaf, so return the prediction of leaf nodes. if not descend, just return a 0-1 array containing the the decision of this node ''' if y is None : y = np . ones ( X . shape [ 0 ]) # dummy yhat = np . ones_like ( y ) if self . decision_boundary is None : # this node is a dummy return self . left () * yhat mask = X [:, self . decision_feature ] > self . decision_boundary if descend : yhat [ mask ] = self . right ( X [ mask ], y [ mask ], descend = True ) yhat [ ~ mask ] = self . left ( X [ ~ mask ], y [ ~ mask ], descend = True ) return yhat else : return mask @staticmethod def delta_gini ( x , y , boundary , gini0 ): '''compute the drop in gini index (with respect to gini0) if we place a decision boundary at boundary''' mask = x > boundary gini_hi = BinaryDecisionNode . gini_index ( y [ mask ]) gini_lo = BinaryDecisionNode . gini_index ( y [ ~ mask ]) frac = mask . astype ( int ) . sum () / mask . shape [ 0 ] delta = gini0 - ( frac * gini_hi ) - (( 1 - frac ) * gini_lo ) return delta @staticmethod def max_delta_gini ( x , y , lo , hi , n_search_pts , gini0 ): '''grid search to maximize the drop in gini index (with respect to gini0) in a specified range Returns: - decision boundary that maximizes delta gini, delta gini''' search_pts = np . linspace ( lo , hi , n_search_pts ) best_delta = 0 best_threshold = lo for bndry in search_pts : new_delta = BinaryDecisionNode . delta_gini ( x , y , bndry , gini0 ) if new_delta > best_delta : best_delta = new_delta best_threshold = bndry return best_threshold , best_delta @staticmethod def gini_index ( y ): '''binary gini index''' p0 = ( y == 0 ) . sum () / ( y . shape [ 0 ] + EPS ) return 1 - p0 ** 2 - ( 1 - p0 ) ** 2 def __str__ ( self ): ''' recursively write a descriptive string for this node and its children. useful for debugging ''' def parse_child ( s ): s = s . split ( ' \\n ' ) s = s [: 1 ] + [ ' ' + ss for ss in s [ 1 :]] return ' \\n ' . join ( s ) s = '' if self . decision_feature is None : s += f 'Node(pred= { self . left () } )' else : s += f 'Node(ft= { self . decision_feature } , bndry= { self . decision_boundary } ' if isinstance ( self . left , BinaryDecisionNode ): s += f ', \\n L: { parse_child ( str ( self . left )) } ' s += f ', \\n R: { parse_child ( str ( self . right )) } ' else : s += f ', L= { str ( self . left ()) } ' s += f ', R= { str ( self . right ()) } ' s += ')' return s class BinaryDecisionTree : ''' Binary decision tree. Parameters: - node_depth (int): maximum depth of the tree - n_search_pts (int): number of points in the search grid - max_features_per_node (int): maximum number of features each node is allowed to use ''' def __init__ ( self , node_depth , n_search_pts = 100 , max_features_per_node = None ): self . node_depth = node_depth self . max_features = max_features_per_node self . n_search_pts = n_search_pts self . root = None def fit ( self , X , y ): ''' Fit the tree Parameters: - X (ndarray): features - y (ndaray): labels ''' self . root = BinaryDecisionNode ( X , y , self . node_depth , n_search_pts = self . n_search_pts , max_features = self . max_features ) def predict ( self , X ): ''' Run inference Parameters - X (ndarray): features ''' assert ( self . root is not None ), ( 'Trying to run predict on a tree that has not been fit!' ) return self . root ( X ) def __str__ ( self ): return str ( self . root ) class RandomForest : ''' Random forest Parameters: - n_trees (int): number of trees - bagging_frac (int): fraction of training set used for training each tree - node_depth (int): maximum tree depth - n_search_pts (int): number of points used to search for optimal decision threshold - max_features_per_node (int): maximum features each node is allowed to use ''' def __init__ ( self , n_trees , bagging_frac , node_depth , n_search_pts , max_features_per_node ): self . trees = [ BinaryDecisionTree ( node_depth , n_search_pts , max_features_per_node ) for _ in range ( n_trees )] self . bagging_frac = bagging_frac def fit ( self , X , y ): ''' Fit the forest Parameters: - X (ndarray): features - y (ndaray): labels ''' for tree in self . trees : N = 0 while N == 0 : mask = np . random . binomial ( 1 , size = X . shape [ 0 ], p = self . bagging_frac ) . astype ( bool ) N = mask . shape [ 0 ] # make sure we never sample an empty set tree . fit ( X [ mask , :], y [ mask ]) def predict ( self , X ): ''' Run inference Parameters - X (ndarray): features ''' return np . mean ([ tree . predict ( X ) for tree in self . trees ], axis = 0 ) def __str__ ( self ): return ' \\n ' . join ([ str ( t ) for t in self . trees ]) if __name__ == '__main__' : x = np . array ([[ 0 , 1 ], [ 1 , 2 ], [ 2 , 3 ], [ 2 , 3 ], [ 3 , 4 ], [ 4 , 5 ]]) y = np . array ([ 0 , 0 , 0 , 1 , 1 , 0 ]) tree = BinaryDecisionTree ( node_depth = 5 ) tree . fit ( x , y ) print ( str ( tree )) yhat = tree . predict ( x ) print ( np . stack ([ y , yhat ], axis =- 1 )) print () forest = RandomForest ( 6 , 0.5 , 1 , 1 ) forest . fit ( x , y ) print ( str ( forest )) yhat = forest . predict ( x ) print ( np . stack ([ y , yhat ], axis =- 1 )) Variables EPS Classes BinaryDecisionNode class BinaryDecisionNode ( X , y , node_depth = 0 , n_search_pts = 100 , max_features = None ) View Source class BinaryDecisionNode : def __init__ ( self , X , y , node_depth = 0 , n_search_pts = 100 , max_features = None ) : ''' Binary decision node. On construction, the BinaryDecisionNode fits itself to the provided data. If node_depth > 0, it will create and fit two nodes on the results of this node Parameters: - X (ndarray): features - y (ndarray): binary labels - node_depth (int): number of nodes to descend - n_search_pts (int): number of points in the grid used to search for an optimal decision threshold - max_features (int): maximum number of features to optimize over at each node ''' # we assume data is in the form ( samples , features ) # and labels are a 1 D binary vector assert len ( X . shape ) == 2 assert len ( y . shape ) == 1 self . decision_boundary = None self . decision_feature = None # randomly sample at most max_features for fitting if max_features is None : features = np . arange ( X . shape [ 1 ] ) else : features = np . random . permutation ( X . shape [ 1 ] ) [ :max_features ] # gini index of the data fed into this node gini0 = self . gini_index ( y ) best_delta = 0 for i_ft in features : # iterate over the possible features , find the best # boundary and the corresponding gini index . pick the # choice of i_ft and boundary that maximizes the drop # from gini0 ( new_delta ) x = X [ :,i_ft ] boundary , new_delta = self . max_delta_gini ( x = x , y = y , lo = np . min ( x ), hi = np . max ( x ), n_search_pts = n_search_pts , gini0 = gini0 ) if new_delta > best_delta : best_delta = new_delta self . decision_feature = i_ft self . decision_boundary = boundary leaf = True # whether this node is a leaf in the tree if self . decision_boundary is not None : # we found a valid decision boundary # run inference on this node to split the dataset . mask = self ( X , descend = False ) # save the mean values of the left and right splits . # not strictly needed if this is not a leaf , but it 's # useful for debugging. could also be useful if we ever # prune the tree. self.right_val = y[mask].sum() / (y[mask].shape[0] + EPS) self.left_val = y[~mask].sum() / (y[~mask].shape[0] + EPS) if node_depth > 0: # if we haven' t reached the max node depth , fit left # and right nodes : self . left = BinaryDecisionNode ( X [ ~mask ] , y [ ~mask ] , node_depth = node_depth - 1 , n_search_pts = n_search_pts ) self . right = BinaryDecisionNode ( X [ mask ] , y [ mask ] , node_depth = node_depth - 1 , n_search_pts = n_search_pts ) leaf = False # has children -> not a leaf else : # no valid decision boundary , so left = right . use the mean # of the current dataset as the prediction for this node self . left_val = self . right_val = np . mean ( y ) if leaf : # if no children , make the 'children' the mean value in each category self . right = partial ( _return_const , val = self . right_val ) self . left = partial ( _return_const , val = self . left_val ) def __call__ ( self , X , y = None , descend = True ) : ''' infer the decision node on a dataset. if descend=true, we traverse the children until we hit leaves. otherwise, just return a boolean array of the decision at this node Parameters: - X (ndarray): features - y (ndarray): labels, optional - descend (boolean): should we descend down into children nodes or not Returns: - ndarray: if descend, we guarantee to end at a leaf, so return the prediction of leaf nodes. if not descend, just return a 0-1 array containing the the decision of this node ''' if y is None : y = np . ones ( X . shape [ 0 ] ) # dummy yhat = np . ones_like ( y ) if self . decision_boundary is None : # this node is a dummy return self . left () * yhat mask = X [ :, self.decision_feature ] > self . decision_boundary if descend : yhat [ mask ] = self . right ( X [ mask ] , y [ mask ] , descend = True ) yhat [ ~mask ] = self . left ( X [ ~mask ] , y [ ~mask ] , descend = True ) return yhat else : return mask @staticmethod def delta_gini ( x , y , boundary , gini0 ) : '''compute the drop in gini index (with respect to gini0) if we place a decision boundary at boundary''' mask = x > boundary gini_hi = BinaryDecisionNode . gini_index ( y [ mask ] ) gini_lo = BinaryDecisionNode . gini_index ( y [ ~mask ] ) frac = mask . astype ( int ). sum () / mask . shape [ 0 ] delta = gini0 - ( frac * gini_hi ) - (( 1 - frac ) * gini_lo ) return delta @staticmethod def max_delta_gini ( x , y , lo , hi , n_search_pts , gini0 ) : '''grid search to maximize the drop in gini index (with respect to gini0) in a specified range Returns: - decision boundary that maximizes delta gini, delta gini''' search_pts = np . linspace ( lo , hi , n_search_pts ) best_delta = 0 best_threshold = lo for bndry in search_pts : new_delta = BinaryDecisionNode . delta_gini ( x , y , bndry , gini0 ) if new_delta > best_delta : best_delta = new_delta best_threshold = bndry return best_threshold , best_delta @staticmethod def gini_index ( y ) : '''binary gini index''' p0 = ( y == 0 ). sum () / ( y . shape [ 0 ] + EPS ) return 1 - p0 ** 2 - ( 1 - p0 ) ** 2 def __str__ ( self ) : ''' recursively write a descriptive string for this node and its children. useful for debugging ''' def parse_child ( s ) : s = s . split ( '\\n' ) s = s [ :1 ] + [ ' ' + ss for ss in s[1: ] ] return '\\n' . join ( s ) s = '' if self . decision_feature is None : s += f 'Node(pred={self.left()})' else : s += f 'Node(ft={self.decision_feature}, bndry={self.decision_boundary}' if isinstance ( self . left , BinaryDecisionNode ) : s += f ',\\n L: {parse_child(str(self.left))}' s += f ',\\n R: {parse_child(str(self.right))}' else : s += f ', L={str(self.left())}' s += f ', R={str(self.right())}' s += ')' return s Static methods delta_gini def delta_gini ( x , y , boundary , gini0 ) compute the drop in gini index (with respect to gini0) if we place a decision boundary at boundary View Source @staticmethod def delta_gini ( x , y , boundary , gini0 ) : '''compute the drop in gini index (with respect to gini0) if we place a decision boundary at boundary''' mask = x > boundary gini_hi = BinaryDecisionNode . gini_index ( y [ mask ] ) gini_lo = BinaryDecisionNode . gini_index ( y [ ~mask ] ) frac = mask . astype ( int ). sum () / mask . shape [ 0 ] delta = gini0 - ( frac * gini_hi ) - (( 1 - frac ) * gini_lo ) return delta gini_index def gini_index ( y ) binary gini index View Source @staticmethod def gini_index ( y ) : '''binary gini index''' p0 = ( y == 0 ). sum () / ( y . shape [ 0 ] + EPS ) return 1 - p0 ** 2 - ( 1 - p0 ) ** 2 max_delta_gini def max_delta_gini ( x , y , lo , hi , n_search_pts , gini0 ) grid search to maximize the drop in gini index (with respect to gini0) in a specified range Returns: decision boundary that maximizes delta gini, delta gini View Source @staticmethod def max_delta_gini ( x , y , lo , hi , n_search_pts , gini0 ) : '''grid search to maximize the drop in gini index (with respect to gini0) in a specified range Returns: - decision boundary that maximizes delta gini, delta gini''' search_pts = np . linspace ( lo , hi , n_search_pts ) best_delta = 0 best_threshold = lo for bndry in search_pts : new_delta = BinaryDecisionNode . delta_gini ( x , y , bndry , gini0 ) if new_delta > best_delta : best_delta = new_delta best_threshold = bndry return best_threshold , best_delta BinaryDecisionTree class BinaryDecisionTree ( node_depth , n_search_pts = 100 , max_features_per_node = None ) Binary decision tree. Parameters: node_depth (int): maximum depth of the tree n_search_pts (int): number of points in the search grid max_features_per_node (int): maximum number of features each node is allowed to use View Source class BinaryDecisionTree: ''' Binary decision tree. Parameters: - node_depth (int): maximum depth of the tree - n_search_pts (int): number of points in the search grid - max_features_per_node (int): maximum number of features each node is allowed to use ''' def __init__ ( self , node_depth , n_search_pts = 100 , max_features_per_node = None ): self . node_depth = node_depth self . max_features = max_features_per_node self . n_search_pts = n_search_pts self . root = None def fit ( self , X , y ): ''' Fit the tree Parameters: - X (ndarray): features - y (ndaray): labels ''' self . root = BinaryDecisionNode ( X , y , self . node_depth , n_search_pts = self . n_search_pts , max_features = self . max_features ) def predict ( self , X ): ''' Run inference Parameters - X (ndarray): features ''' assert ( self . root is not None ), ( 'Trying to run predict on a tree that has not been fit!' ) return self . root ( X ) def __str__ ( self ): return str ( self . root ) Methods fit def fit ( self , X , y ) Fit the tree Parameters: X (ndarray): features y (ndaray): labels View Source def fit ( self , X , y ): ''' Fit the tree Parameters: - X (ndarray): features - y (ndaray): labels ''' self . root = BinaryDecisionNode ( X , y , self . node_depth , n_search_pts = self . n_search_pts , max_features = self . max_features ) predict def predict ( self , X ) Run inference Parameters X (ndarray): features View Source def predict ( self , X ): ''' Run inference Parameters - X (ndarray): features ''' assert ( self . root is not None ), ( 'Trying to run predict on a tree that has not been fit!' ) return self . root ( X ) RandomForest class RandomForest ( n_trees , bagging_frac , node_depth , n_search_pts , max_features_per_node ) Random forest Parameters: n_trees (int): number of trees bagging_frac (int): fraction of training set used for training each tree node_depth (int): maximum tree depth n_search_pts (int): number of points used to search for optimal decision threshold max_features_per_node (int): maximum features each node is allowed to use View Source class RandomForest : ''' Random forest Parameters: - n_trees (int): number of trees - bagging_frac (int): fraction of training set used for training each tree - node_depth (int): maximum tree depth - n_search_pts (int): number of points used to search for optimal decision threshold - max_features_per_node (int): maximum features each node is allowed to use ''' def __init__ ( self , n_trees , bagging_frac , node_depth , n_search_pts , max_features_per_node ) : self . trees = [ BinaryDecisionTree(node_depth, n_search_pts, max_features_per_node) for _ in range(n_trees) ] self . bagging_frac = bagging_frac def fit ( self , X , y ) : ''' Fit the forest Parameters: - X (ndarray): features - y (ndaray): labels ''' for tree in self . trees : N = 0 while N == 0 : mask = np . random . binomial ( 1 , size = X . shape [ 0 ] , p = self . bagging_frac ). astype ( bool ) N = mask . shape [ 0 ] # make sure we never sample an empty set tree . fit ( X [ mask, : ] , y [ mask ] ) def predict ( self , X ) : ''' Run inference Parameters - X (ndarray): features ''' return np . mean ( [ tree.predict(X) for tree in self.trees ] , axis = 0 ) def __str__ ( self ) : return '\\n' . join ( [ str(t) for t in self.trees ] ) Methods fit def fit ( self , X , y ) Fit the forest Parameters: X (ndarray): features y (ndaray): labels View Source def fit ( self , X , y ) : ''' Fit the forest Parameters: - X (ndarray): features - y (ndaray): labels ''' for tree in self . trees : N = 0 while N == 0 : mask = np . random . binomial ( 1 , size = X . shape [ 0 ] , p = self . bagging_frac ). astype ( bool ) N = mask . shape [ 0 ] # make sure we never sample an empty set tree . fit ( X [ mask, : ] , y [ mask ] ) predict def predict ( self , X ) Run inference Parameters X (ndarray): features View Source def predict ( self , X ): ''' Run inference Parameters - X (ndarray): features ''' return np . mean ([ tree . predict ( X ) for tree in self . trees ], axis = 0 )","title":"Forest"},{"location":"reference/braf/forest/#module-brafforest","text":"View Source import numpy as np import tqdm from functools import partial EPS = 1e-12 # put this in top-level scope so it can be pickled def _return_const ( * args , ** kwargs ): return kwargs [ 'val' ] class BinaryDecisionNode : def __init__ ( self , X , y , node_depth = 0 , n_search_pts = 100 , max_features = None ): ''' Binary decision node. On construction, the BinaryDecisionNode fits itself to the provided data. If node_depth > 0, it will create and fit two nodes on the results of this node Parameters: - X (ndarray): features - y (ndarray): binary labels - node_depth (int): number of nodes to descend - n_search_pts (int): number of points in the grid used to search for an optimal decision threshold - max_features (int): maximum number of features to optimize over at each node ''' # we assume data is in the form (samples, features) # and labels are a 1D binary vector assert len ( X . shape ) == 2 assert len ( y . shape ) == 1 self . decision_boundary = None self . decision_feature = None # randomly sample at most max_features for fitting if max_features is None : features = np . arange ( X . shape [ 1 ]) else : features = np . random . permutation ( X . shape [ 1 ])[: max_features ] # gini index of the data fed into this node gini0 = self . gini_index ( y ) best_delta = 0 for i_ft in features : # iterate over the possible features, find the best # boundary and the corresponding gini index. pick the # choice of i_ft and boundary that maximizes the drop # from gini0 (new_delta) x = X [:, i_ft ] boundary , new_delta = self . max_delta_gini ( x = x , y = y , lo = np . min ( x ), hi = np . max ( x ), n_search_pts = n_search_pts , gini0 = gini0 ) if new_delta > best_delta : best_delta = new_delta self . decision_feature = i_ft self . decision_boundary = boundary leaf = True # whether this node is a leaf in the tree if self . decision_boundary is not None : # we found a valid decision boundary # run inference on this node to split the dataset. mask = self ( X , descend = False ) # save the mean values of the left and right splits. # not strictly needed if this is not a leaf, but it's # useful for debugging. could also be useful if we ever # prune the tree. self . right_val = y [ mask ] . sum () / ( y [ mask ] . shape [ 0 ] + EPS ) self . left_val = y [ ~ mask ] . sum () / ( y [ ~ mask ] . shape [ 0 ] + EPS ) if node_depth > 0 : # if we haven't reached the max node depth, fit left # and right nodes: self . left = BinaryDecisionNode ( X [ ~ mask ], y [ ~ mask ], node_depth = node_depth - 1 , n_search_pts = n_search_pts ) self . right = BinaryDecisionNode ( X [ mask ], y [ mask ], node_depth = node_depth - 1 , n_search_pts = n_search_pts ) leaf = False # has children -> not a leaf else : # no valid decision boundary, so left = right. use the mean # of the current dataset as the prediction for this node self . left_val = self . right_val = np . mean ( y ) if leaf : # if no children, make the 'children' the mean value in each category self . right = partial ( _return_const , val = self . right_val ) self . left = partial ( _return_const , val = self . left_val ) def __call__ ( self , X , y = None , descend = True ): ''' infer the decision node on a dataset. if descend=true, we traverse the children until we hit leaves. otherwise, just return a boolean array of the decision at this node Parameters: - X (ndarray): features - y (ndarray): labels, optional - descend (boolean): should we descend down into children nodes or not Returns: - ndarray: if descend, we guarantee to end at a leaf, so return the prediction of leaf nodes. if not descend, just return a 0-1 array containing the the decision of this node ''' if y is None : y = np . ones ( X . shape [ 0 ]) # dummy yhat = np . ones_like ( y ) if self . decision_boundary is None : # this node is a dummy return self . left () * yhat mask = X [:, self . decision_feature ] > self . decision_boundary if descend : yhat [ mask ] = self . right ( X [ mask ], y [ mask ], descend = True ) yhat [ ~ mask ] = self . left ( X [ ~ mask ], y [ ~ mask ], descend = True ) return yhat else : return mask @staticmethod def delta_gini ( x , y , boundary , gini0 ): '''compute the drop in gini index (with respect to gini0) if we place a decision boundary at boundary''' mask = x > boundary gini_hi = BinaryDecisionNode . gini_index ( y [ mask ]) gini_lo = BinaryDecisionNode . gini_index ( y [ ~ mask ]) frac = mask . astype ( int ) . sum () / mask . shape [ 0 ] delta = gini0 - ( frac * gini_hi ) - (( 1 - frac ) * gini_lo ) return delta @staticmethod def max_delta_gini ( x , y , lo , hi , n_search_pts , gini0 ): '''grid search to maximize the drop in gini index (with respect to gini0) in a specified range Returns: - decision boundary that maximizes delta gini, delta gini''' search_pts = np . linspace ( lo , hi , n_search_pts ) best_delta = 0 best_threshold = lo for bndry in search_pts : new_delta = BinaryDecisionNode . delta_gini ( x , y , bndry , gini0 ) if new_delta > best_delta : best_delta = new_delta best_threshold = bndry return best_threshold , best_delta @staticmethod def gini_index ( y ): '''binary gini index''' p0 = ( y == 0 ) . sum () / ( y . shape [ 0 ] + EPS ) return 1 - p0 ** 2 - ( 1 - p0 ) ** 2 def __str__ ( self ): ''' recursively write a descriptive string for this node and its children. useful for debugging ''' def parse_child ( s ): s = s . split ( ' \\n ' ) s = s [: 1 ] + [ ' ' + ss for ss in s [ 1 :]] return ' \\n ' . join ( s ) s = '' if self . decision_feature is None : s += f 'Node(pred= { self . left () } )' else : s += f 'Node(ft= { self . decision_feature } , bndry= { self . decision_boundary } ' if isinstance ( self . left , BinaryDecisionNode ): s += f ', \\n L: { parse_child ( str ( self . left )) } ' s += f ', \\n R: { parse_child ( str ( self . right )) } ' else : s += f ', L= { str ( self . left ()) } ' s += f ', R= { str ( self . right ()) } ' s += ')' return s class BinaryDecisionTree : ''' Binary decision tree. Parameters: - node_depth (int): maximum depth of the tree - n_search_pts (int): number of points in the search grid - max_features_per_node (int): maximum number of features each node is allowed to use ''' def __init__ ( self , node_depth , n_search_pts = 100 , max_features_per_node = None ): self . node_depth = node_depth self . max_features = max_features_per_node self . n_search_pts = n_search_pts self . root = None def fit ( self , X , y ): ''' Fit the tree Parameters: - X (ndarray): features - y (ndaray): labels ''' self . root = BinaryDecisionNode ( X , y , self . node_depth , n_search_pts = self . n_search_pts , max_features = self . max_features ) def predict ( self , X ): ''' Run inference Parameters - X (ndarray): features ''' assert ( self . root is not None ), ( 'Trying to run predict on a tree that has not been fit!' ) return self . root ( X ) def __str__ ( self ): return str ( self . root ) class RandomForest : ''' Random forest Parameters: - n_trees (int): number of trees - bagging_frac (int): fraction of training set used for training each tree - node_depth (int): maximum tree depth - n_search_pts (int): number of points used to search for optimal decision threshold - max_features_per_node (int): maximum features each node is allowed to use ''' def __init__ ( self , n_trees , bagging_frac , node_depth , n_search_pts , max_features_per_node ): self . trees = [ BinaryDecisionTree ( node_depth , n_search_pts , max_features_per_node ) for _ in range ( n_trees )] self . bagging_frac = bagging_frac def fit ( self , X , y ): ''' Fit the forest Parameters: - X (ndarray): features - y (ndaray): labels ''' for tree in self . trees : N = 0 while N == 0 : mask = np . random . binomial ( 1 , size = X . shape [ 0 ], p = self . bagging_frac ) . astype ( bool ) N = mask . shape [ 0 ] # make sure we never sample an empty set tree . fit ( X [ mask , :], y [ mask ]) def predict ( self , X ): ''' Run inference Parameters - X (ndarray): features ''' return np . mean ([ tree . predict ( X ) for tree in self . trees ], axis = 0 ) def __str__ ( self ): return ' \\n ' . join ([ str ( t ) for t in self . trees ]) if __name__ == '__main__' : x = np . array ([[ 0 , 1 ], [ 1 , 2 ], [ 2 , 3 ], [ 2 , 3 ], [ 3 , 4 ], [ 4 , 5 ]]) y = np . array ([ 0 , 0 , 0 , 1 , 1 , 0 ]) tree = BinaryDecisionTree ( node_depth = 5 ) tree . fit ( x , y ) print ( str ( tree )) yhat = tree . predict ( x ) print ( np . stack ([ y , yhat ], axis =- 1 )) print () forest = RandomForest ( 6 , 0.5 , 1 , 1 ) forest . fit ( x , y ) print ( str ( forest )) yhat = forest . predict ( x ) print ( np . stack ([ y , yhat ], axis =- 1 ))","title":"Module braf.forest"},{"location":"reference/braf/forest/#variables","text":"EPS","title":"Variables"},{"location":"reference/braf/forest/#classes","text":"","title":"Classes"},{"location":"reference/braf/forest/#binarydecisionnode","text":"class BinaryDecisionNode ( X , y , node_depth = 0 , n_search_pts = 100 , max_features = None ) View Source class BinaryDecisionNode : def __init__ ( self , X , y , node_depth = 0 , n_search_pts = 100 , max_features = None ) : ''' Binary decision node. On construction, the BinaryDecisionNode fits itself to the provided data. If node_depth > 0, it will create and fit two nodes on the results of this node Parameters: - X (ndarray): features - y (ndarray): binary labels - node_depth (int): number of nodes to descend - n_search_pts (int): number of points in the grid used to search for an optimal decision threshold - max_features (int): maximum number of features to optimize over at each node ''' # we assume data is in the form ( samples , features ) # and labels are a 1 D binary vector assert len ( X . shape ) == 2 assert len ( y . shape ) == 1 self . decision_boundary = None self . decision_feature = None # randomly sample at most max_features for fitting if max_features is None : features = np . arange ( X . shape [ 1 ] ) else : features = np . random . permutation ( X . shape [ 1 ] ) [ :max_features ] # gini index of the data fed into this node gini0 = self . gini_index ( y ) best_delta = 0 for i_ft in features : # iterate over the possible features , find the best # boundary and the corresponding gini index . pick the # choice of i_ft and boundary that maximizes the drop # from gini0 ( new_delta ) x = X [ :,i_ft ] boundary , new_delta = self . max_delta_gini ( x = x , y = y , lo = np . min ( x ), hi = np . max ( x ), n_search_pts = n_search_pts , gini0 = gini0 ) if new_delta > best_delta : best_delta = new_delta self . decision_feature = i_ft self . decision_boundary = boundary leaf = True # whether this node is a leaf in the tree if self . decision_boundary is not None : # we found a valid decision boundary # run inference on this node to split the dataset . mask = self ( X , descend = False ) # save the mean values of the left and right splits . # not strictly needed if this is not a leaf , but it 's # useful for debugging. could also be useful if we ever # prune the tree. self.right_val = y[mask].sum() / (y[mask].shape[0] + EPS) self.left_val = y[~mask].sum() / (y[~mask].shape[0] + EPS) if node_depth > 0: # if we haven' t reached the max node depth , fit left # and right nodes : self . left = BinaryDecisionNode ( X [ ~mask ] , y [ ~mask ] , node_depth = node_depth - 1 , n_search_pts = n_search_pts ) self . right = BinaryDecisionNode ( X [ mask ] , y [ mask ] , node_depth = node_depth - 1 , n_search_pts = n_search_pts ) leaf = False # has children -> not a leaf else : # no valid decision boundary , so left = right . use the mean # of the current dataset as the prediction for this node self . left_val = self . right_val = np . mean ( y ) if leaf : # if no children , make the 'children' the mean value in each category self . right = partial ( _return_const , val = self . right_val ) self . left = partial ( _return_const , val = self . left_val ) def __call__ ( self , X , y = None , descend = True ) : ''' infer the decision node on a dataset. if descend=true, we traverse the children until we hit leaves. otherwise, just return a boolean array of the decision at this node Parameters: - X (ndarray): features - y (ndarray): labels, optional - descend (boolean): should we descend down into children nodes or not Returns: - ndarray: if descend, we guarantee to end at a leaf, so return the prediction of leaf nodes. if not descend, just return a 0-1 array containing the the decision of this node ''' if y is None : y = np . ones ( X . shape [ 0 ] ) # dummy yhat = np . ones_like ( y ) if self . decision_boundary is None : # this node is a dummy return self . left () * yhat mask = X [ :, self.decision_feature ] > self . decision_boundary if descend : yhat [ mask ] = self . right ( X [ mask ] , y [ mask ] , descend = True ) yhat [ ~mask ] = self . left ( X [ ~mask ] , y [ ~mask ] , descend = True ) return yhat else : return mask @staticmethod def delta_gini ( x , y , boundary , gini0 ) : '''compute the drop in gini index (with respect to gini0) if we place a decision boundary at boundary''' mask = x > boundary gini_hi = BinaryDecisionNode . gini_index ( y [ mask ] ) gini_lo = BinaryDecisionNode . gini_index ( y [ ~mask ] ) frac = mask . astype ( int ). sum () / mask . shape [ 0 ] delta = gini0 - ( frac * gini_hi ) - (( 1 - frac ) * gini_lo ) return delta @staticmethod def max_delta_gini ( x , y , lo , hi , n_search_pts , gini0 ) : '''grid search to maximize the drop in gini index (with respect to gini0) in a specified range Returns: - decision boundary that maximizes delta gini, delta gini''' search_pts = np . linspace ( lo , hi , n_search_pts ) best_delta = 0 best_threshold = lo for bndry in search_pts : new_delta = BinaryDecisionNode . delta_gini ( x , y , bndry , gini0 ) if new_delta > best_delta : best_delta = new_delta best_threshold = bndry return best_threshold , best_delta @staticmethod def gini_index ( y ) : '''binary gini index''' p0 = ( y == 0 ). sum () / ( y . shape [ 0 ] + EPS ) return 1 - p0 ** 2 - ( 1 - p0 ) ** 2 def __str__ ( self ) : ''' recursively write a descriptive string for this node and its children. useful for debugging ''' def parse_child ( s ) : s = s . split ( '\\n' ) s = s [ :1 ] + [ ' ' + ss for ss in s[1: ] ] return '\\n' . join ( s ) s = '' if self . decision_feature is None : s += f 'Node(pred={self.left()})' else : s += f 'Node(ft={self.decision_feature}, bndry={self.decision_boundary}' if isinstance ( self . left , BinaryDecisionNode ) : s += f ',\\n L: {parse_child(str(self.left))}' s += f ',\\n R: {parse_child(str(self.right))}' else : s += f ', L={str(self.left())}' s += f ', R={str(self.right())}' s += ')' return s","title":"BinaryDecisionNode"},{"location":"reference/braf/forest/#static-methods","text":"","title":"Static methods"},{"location":"reference/braf/forest/#delta_gini","text":"def delta_gini ( x , y , boundary , gini0 ) compute the drop in gini index (with respect to gini0) if we place a decision boundary at boundary View Source @staticmethod def delta_gini ( x , y , boundary , gini0 ) : '''compute the drop in gini index (with respect to gini0) if we place a decision boundary at boundary''' mask = x > boundary gini_hi = BinaryDecisionNode . gini_index ( y [ mask ] ) gini_lo = BinaryDecisionNode . gini_index ( y [ ~mask ] ) frac = mask . astype ( int ). sum () / mask . shape [ 0 ] delta = gini0 - ( frac * gini_hi ) - (( 1 - frac ) * gini_lo ) return delta","title":"delta_gini"},{"location":"reference/braf/forest/#gini_index","text":"def gini_index ( y ) binary gini index View Source @staticmethod def gini_index ( y ) : '''binary gini index''' p0 = ( y == 0 ). sum () / ( y . shape [ 0 ] + EPS ) return 1 - p0 ** 2 - ( 1 - p0 ) ** 2","title":"gini_index"},{"location":"reference/braf/forest/#max_delta_gini","text":"def max_delta_gini ( x , y , lo , hi , n_search_pts , gini0 ) grid search to maximize the drop in gini index (with respect to gini0) in a specified range Returns: decision boundary that maximizes delta gini, delta gini View Source @staticmethod def max_delta_gini ( x , y , lo , hi , n_search_pts , gini0 ) : '''grid search to maximize the drop in gini index (with respect to gini0) in a specified range Returns: - decision boundary that maximizes delta gini, delta gini''' search_pts = np . linspace ( lo , hi , n_search_pts ) best_delta = 0 best_threshold = lo for bndry in search_pts : new_delta = BinaryDecisionNode . delta_gini ( x , y , bndry , gini0 ) if new_delta > best_delta : best_delta = new_delta best_threshold = bndry return best_threshold , best_delta","title":"max_delta_gini"},{"location":"reference/braf/forest/#binarydecisiontree","text":"class BinaryDecisionTree ( node_depth , n_search_pts = 100 , max_features_per_node = None ) Binary decision tree. Parameters: node_depth (int): maximum depth of the tree n_search_pts (int): number of points in the search grid max_features_per_node (int): maximum number of features each node is allowed to use View Source class BinaryDecisionTree: ''' Binary decision tree. Parameters: - node_depth (int): maximum depth of the tree - n_search_pts (int): number of points in the search grid - max_features_per_node (int): maximum number of features each node is allowed to use ''' def __init__ ( self , node_depth , n_search_pts = 100 , max_features_per_node = None ): self . node_depth = node_depth self . max_features = max_features_per_node self . n_search_pts = n_search_pts self . root = None def fit ( self , X , y ): ''' Fit the tree Parameters: - X (ndarray): features - y (ndaray): labels ''' self . root = BinaryDecisionNode ( X , y , self . node_depth , n_search_pts = self . n_search_pts , max_features = self . max_features ) def predict ( self , X ): ''' Run inference Parameters - X (ndarray): features ''' assert ( self . root is not None ), ( 'Trying to run predict on a tree that has not been fit!' ) return self . root ( X ) def __str__ ( self ): return str ( self . root )","title":"BinaryDecisionTree"},{"location":"reference/braf/forest/#methods","text":"","title":"Methods"},{"location":"reference/braf/forest/#fit","text":"def fit ( self , X , y ) Fit the tree Parameters: X (ndarray): features y (ndaray): labels View Source def fit ( self , X , y ): ''' Fit the tree Parameters: - X (ndarray): features - y (ndaray): labels ''' self . root = BinaryDecisionNode ( X , y , self . node_depth , n_search_pts = self . n_search_pts , max_features = self . max_features )","title":"fit"},{"location":"reference/braf/forest/#predict","text":"def predict ( self , X ) Run inference Parameters X (ndarray): features View Source def predict ( self , X ): ''' Run inference Parameters - X (ndarray): features ''' assert ( self . root is not None ), ( 'Trying to run predict on a tree that has not been fit!' ) return self . root ( X )","title":"predict"},{"location":"reference/braf/forest/#randomforest","text":"class RandomForest ( n_trees , bagging_frac , node_depth , n_search_pts , max_features_per_node ) Random forest Parameters: n_trees (int): number of trees bagging_frac (int): fraction of training set used for training each tree node_depth (int): maximum tree depth n_search_pts (int): number of points used to search for optimal decision threshold max_features_per_node (int): maximum features each node is allowed to use View Source class RandomForest : ''' Random forest Parameters: - n_trees (int): number of trees - bagging_frac (int): fraction of training set used for training each tree - node_depth (int): maximum tree depth - n_search_pts (int): number of points used to search for optimal decision threshold - max_features_per_node (int): maximum features each node is allowed to use ''' def __init__ ( self , n_trees , bagging_frac , node_depth , n_search_pts , max_features_per_node ) : self . trees = [ BinaryDecisionTree(node_depth, n_search_pts, max_features_per_node) for _ in range(n_trees) ] self . bagging_frac = bagging_frac def fit ( self , X , y ) : ''' Fit the forest Parameters: - X (ndarray): features - y (ndaray): labels ''' for tree in self . trees : N = 0 while N == 0 : mask = np . random . binomial ( 1 , size = X . shape [ 0 ] , p = self . bagging_frac ). astype ( bool ) N = mask . shape [ 0 ] # make sure we never sample an empty set tree . fit ( X [ mask, : ] , y [ mask ] ) def predict ( self , X ) : ''' Run inference Parameters - X (ndarray): features ''' return np . mean ( [ tree.predict(X) for tree in self.trees ] , axis = 0 ) def __str__ ( self ) : return '\\n' . join ( [ str(t) for t in self.trees ] )","title":"RandomForest"},{"location":"reference/braf/forest/#methods_1","text":"","title":"Methods"},{"location":"reference/braf/forest/#fit_1","text":"def fit ( self , X , y ) Fit the forest Parameters: X (ndarray): features y (ndaray): labels View Source def fit ( self , X , y ) : ''' Fit the forest Parameters: - X (ndarray): features - y (ndaray): labels ''' for tree in self . trees : N = 0 while N == 0 : mask = np . random . binomial ( 1 , size = X . shape [ 0 ] , p = self . bagging_frac ). astype ( bool ) N = mask . shape [ 0 ] # make sure we never sample an empty set tree . fit ( X [ mask, : ] , y [ mask ] )","title":"fit"},{"location":"reference/braf/forest/#predict_1","text":"def predict ( self , X ) Run inference Parameters X (ndarray): features View Source def predict ( self , X ): ''' Run inference Parameters - X (ndarray): features ''' return np . mean ([ tree . predict ( X ) for tree in self . trees ], axis = 0 )","title":"predict"},{"location":"reference/braf/knn/","text":"Module braf.knn View Source import numpy as np def euclidean_distance_2 ( x0 , x1 ): '''euclidean_distance_2 returns the *squared* euclidean distance between rows of x0 and x1''' # d[i,j] is x0[i] - x1[j] d = x0 [:, None ,:] - x1 [ None ,:,:] d2 = np . square ( d ) metric = np . sum ( d2 , axis =- 1 ) return metric class KNN : ''' A K-NN implementatation that performs in O(n) time for each query Parameters: - k (int): number of nearest neighbors - metric (str): the metric function. only Euclidean is supported at the moment ''' def __init__ ( self , k , metric = 'euclidean' ): self . _k = k if metric == 'euclidean' : self . _metric = euclidean_distance_2 def get_neighbors ( self , X , Z ): ''' compute the k nearest neighbors in Z of elements of X Parameters: - X (ndarray) - Z (ndarray) Returns: - ndarray: an array of indices in Z, of shape (X.shape[0], k) ''' # D[i,j] is the distance between X[i] and Z[j] D = self . _metric ( X , Z ) # k_closest[i] are the indices of the k closest elements of Z to X[i] # note that they are *not* ordered k_closest = np . argpartition ( D , self . _k , axis =- 1 )[:, : self . _k ] return k_closest if __name__ == '__main__' : X = np . array ([[ 0 , 2 ], [ 2 , 3 ]]) Z = np . array ([[ 0 , 2 ], [ 1 , 2 ], [ 3 , 4 ], [ - 1 , 0 ]]) knn = KNN ( 3 ) print ( Z [ knn . get_neighbors ( X , Z )]) Functions euclidean_distance_2 def euclidean_distance_2 ( x0 , x1 ) euclidean_distance_2 returns the squared euclidean distance between rows of x0 and x1 View Source def euclidean_distance_2 ( x0 , x1 ) : '''euclidean_distance_2 returns the *squared* euclidean distance between rows of x0 and x1''' # d [ i,j ] is x0 [ i ] - x1 [ j ] d = x0 [ :,None,: ] - x1 [ None,:,: ] d2 = np . square ( d ) metric = np . sum ( d2 , axis =- 1 ) return metric Classes KNN class KNN ( k , metric = 'euclidean' ) A K-NN implementatation that performs in O(n) time for each query Parameters: k (int): number of nearest neighbors metric (str): the metric function. only Euclidean is supported at the moment View Source class KNN : ''' A K-NN implementatation that performs in O(n) time for each query Parameters: - k (int): number of nearest neighbors - metric (str): the metric function. only Euclidean is supported at the moment ''' def __init__ ( self , k , metric = 'euclidean' ) : self . _k = k if metric == 'euclidean' : self . _metric = euclidean_distance_2 def get_neighbors ( self , X , Z ) : ''' compute the k nearest neighbors in Z of elements of X Parameters: - X (ndarray) - Z (ndarray) Returns: - ndarray: an array of indices in Z, of shape (X.shape[0], k) ''' # D [ i,j ] is the distance between X [ i ] and Z [ j ] D = self . _metric ( X , Z ) # k_closest [ i ] are the indices of the k closest elements of Z to X [ i ] # note that they are * not * ordered k_closest = np . argpartition ( D , self . _k , axis =- 1 ) [ :, :self._k ] return k_closest Methods get_neighbors def get_neighbors ( self , X , Z ) compute the k nearest neighbors in Z of elements of X Parameters: X (ndarray) Z (ndarray) Returns: ndarray: an array of indices in Z, of shape (X.shape[0], k) View Source def get_neighbors ( self , X , Z ) : ''' compute the k nearest neighbors in Z of elements of X Parameters: - X (ndarray) - Z (ndarray) Returns: - ndarray: an array of indices in Z, of shape (X.shape[0], k) ''' # D [ i,j ] is the distance between X [ i ] and Z [ j ] D = self . _metric ( X , Z ) # k_closest [ i ] are the indices of the k closest elements of Z to X [ i ] # note that they are * not * ordered k_closest = np . argpartition ( D , self . _k , axis =- 1 ) [ :, :self._k ] return k_closest","title":"Knn"},{"location":"reference/braf/knn/#module-brafknn","text":"View Source import numpy as np def euclidean_distance_2 ( x0 , x1 ): '''euclidean_distance_2 returns the *squared* euclidean distance between rows of x0 and x1''' # d[i,j] is x0[i] - x1[j] d = x0 [:, None ,:] - x1 [ None ,:,:] d2 = np . square ( d ) metric = np . sum ( d2 , axis =- 1 ) return metric class KNN : ''' A K-NN implementatation that performs in O(n) time for each query Parameters: - k (int): number of nearest neighbors - metric (str): the metric function. only Euclidean is supported at the moment ''' def __init__ ( self , k , metric = 'euclidean' ): self . _k = k if metric == 'euclidean' : self . _metric = euclidean_distance_2 def get_neighbors ( self , X , Z ): ''' compute the k nearest neighbors in Z of elements of X Parameters: - X (ndarray) - Z (ndarray) Returns: - ndarray: an array of indices in Z, of shape (X.shape[0], k) ''' # D[i,j] is the distance between X[i] and Z[j] D = self . _metric ( X , Z ) # k_closest[i] are the indices of the k closest elements of Z to X[i] # note that they are *not* ordered k_closest = np . argpartition ( D , self . _k , axis =- 1 )[:, : self . _k ] return k_closest if __name__ == '__main__' : X = np . array ([[ 0 , 2 ], [ 2 , 3 ]]) Z = np . array ([[ 0 , 2 ], [ 1 , 2 ], [ 3 , 4 ], [ - 1 , 0 ]]) knn = KNN ( 3 ) print ( Z [ knn . get_neighbors ( X , Z )])","title":"Module braf.knn"},{"location":"reference/braf/knn/#functions","text":"","title":"Functions"},{"location":"reference/braf/knn/#euclidean_distance_2","text":"def euclidean_distance_2 ( x0 , x1 ) euclidean_distance_2 returns the squared euclidean distance between rows of x0 and x1 View Source def euclidean_distance_2 ( x0 , x1 ) : '''euclidean_distance_2 returns the *squared* euclidean distance between rows of x0 and x1''' # d [ i,j ] is x0 [ i ] - x1 [ j ] d = x0 [ :,None,: ] - x1 [ None,:,: ] d2 = np . square ( d ) metric = np . sum ( d2 , axis =- 1 ) return metric","title":"euclidean_distance_2"},{"location":"reference/braf/knn/#classes","text":"","title":"Classes"},{"location":"reference/braf/knn/#knn","text":"class KNN ( k , metric = 'euclidean' ) A K-NN implementatation that performs in O(n) time for each query Parameters: k (int): number of nearest neighbors metric (str): the metric function. only Euclidean is supported at the moment View Source class KNN : ''' A K-NN implementatation that performs in O(n) time for each query Parameters: - k (int): number of nearest neighbors - metric (str): the metric function. only Euclidean is supported at the moment ''' def __init__ ( self , k , metric = 'euclidean' ) : self . _k = k if metric == 'euclidean' : self . _metric = euclidean_distance_2 def get_neighbors ( self , X , Z ) : ''' compute the k nearest neighbors in Z of elements of X Parameters: - X (ndarray) - Z (ndarray) Returns: - ndarray: an array of indices in Z, of shape (X.shape[0], k) ''' # D [ i,j ] is the distance between X [ i ] and Z [ j ] D = self . _metric ( X , Z ) # k_closest [ i ] are the indices of the k closest elements of Z to X [ i ] # note that they are * not * ordered k_closest = np . argpartition ( D , self . _k , axis =- 1 ) [ :, :self._k ] return k_closest","title":"KNN"},{"location":"reference/braf/knn/#methods","text":"","title":"Methods"},{"location":"reference/braf/knn/#get_neighbors","text":"def get_neighbors ( self , X , Z ) compute the k nearest neighbors in Z of elements of X Parameters: X (ndarray) Z (ndarray) Returns: ndarray: an array of indices in Z, of shape (X.shape[0], k) View Source def get_neighbors ( self , X , Z ) : ''' compute the k nearest neighbors in Z of elements of X Parameters: - X (ndarray) - Z (ndarray) Returns: - ndarray: an array of indices in Z, of shape (X.shape[0], k) ''' # D [ i,j ] is the distance between X [ i ] and Z [ j ] D = self . _metric ( X , Z ) # k_closest [ i ] are the indices of the k closest elements of Z to X [ i ] # note that they are * not * ordered k_closest = np . argpartition ( D , self . _k , axis =- 1 ) [ :, :self._k ] return k_closest","title":"get_neighbors"},{"location":"reference/braf/plot/","text":"Module braf.plot View Source import numpy as np import matplotlib.pyplot as plt EPS = 1e-12 def _plot_to ( path ): '''save a plot''' plt . tight_layout () # print(f'Saving to {path}') for ext in ( 'pdf' , 'png' ): plt . savefig ( f ' { path } . { ext } ' ) plt . clf () def df_hist ( df , path , suffix ): '''plot all columns as a dataframe as histograms''' df . hist ( bins = 100 ) _plot_to ( f ' { path } /features_ { suffix } ' ) def prediction_hist ( yhat , y , path ): '''plot the prediction scores, assuming two classes''' bins = np . linspace ( 0 , 1 , 25 ) plt . hist ( yhat [ y == 0 ], bins = bins , alpha = 0.5 , label = 'Outcome 0' ) plt . hist ( yhat [ y == 1 ], bins = bins , alpha = 0.5 , label = 'Outcome 1' ) plt . legend () _plot_to ( f ' { path } /yhat' ) def _fp ( yhat , y , c ): '''compute false positives with a decision threshold c''' mask = yhat > c fps = np . logical_and ( mask , y == 0 ) . sum () total = ( y == 0 ) . sum () + EPS return fps / total def _tp ( yhat , y , c ): '''compute true positives with a decision threshold c''' mask = yhat > c tps = np . logical_and ( mask , y == 1 ) . sum () total = ( y == 1 ) . sum () + EPS return tps / total def roc ( yhat , y , path , suffix ): ''' compute ROC curve and return AUC Parameters: - yhat (ndarray): prediction score for the positive class - y (ndarray): binary label - path (str): output directory to which the plot should be saved - suffix (str): label to make the plot filename unique Returns: - float: area under the ROC curve ''' thresholds = np . linspace ( 1 , 0 , 25 ) fp = [ _fp ( yhat , y , t ) for t in thresholds ] tp = [ _tp ( yhat , y , t ) for t in thresholds ] auc = np . trapz ( tp , x = fp ) plt . plot ( fp , tp , label = f 'AUC= { auc : .3f } ' ) plt . xlabel ( 'False Positive Rate' ) plt . ylabel ( 'True Positive Rate' ) plt . legend () _plot_to ( f ' { path } /roc_ { suffix } ' ) return auc def _prec ( yhat , y , c ): '''compute precision with a decision threshold c''' mask = yhat > c tps = np . logical_and ( mask , y == 1 ) . sum () total = mask . sum () + EPS return tps / total def prc ( yhat , y , path , suffix ): ''' compute PRC curve and return AUC Parameters: - yhat (ndarray): prediction score for the positive class - y (ndarray): binary label - path (str): output directory to which the plot should be saved - suffix (str): label to make the plot filename unique Returns: - float: area under the PRC curve ''' thresholds = np . linspace ( 1 , 0 , 25 ) prec = [ _prec ( yhat , y , t ) for t in thresholds ] tp = [ _tp ( yhat , y , t ) for t in thresholds ] auc = np . trapz ( prec , x = tp ) plt . plot ( tp , prec , label = f 'AUC= { auc : .3f } ' ) plt . xlabel ( 'Recall' ) plt . ylabel ( 'Precision' ) plt . legend () _plot_to ( f ' { path } /prc_ { suffix } ' ) return auc , _prec ( yhat , y , 0.5 ), _tp ( yhat , y , 0.5 ) Variables EPS Functions df_hist def df_hist ( df , path , suffix ) plot all columns as a dataframe as histograms View Source def df_hist ( df , path , suffix ): '''plot all columns as a dataframe as histograms''' df . hist ( bins = 100 ) _plot_to ( f '{path}/features_{suffix}' ) prc def prc ( yhat , y , path , suffix ) compute PRC curve and return AUC Parameters: yhat (ndarray): prediction score for the positive class y (ndarray): binary label path (str): output directory to which the plot should be saved suffix (str): label to make the plot filename unique Returns: float: area under the PRC curve View Source def prc ( yhat , y , path , suffix ): ''' compute PRC curve and return AUC Parameters: - yhat (ndarray): prediction score for the positive class - y (ndarray): binary label - path (str): output directory to which the plot should be saved - suffix (str): label to make the plot filename unique Returns: - float: area under the PRC curve ''' thresholds = np . linspace ( 1 , 0 , 25 ) prec = [ _prec ( yhat , y , t ) for t in thresholds ] tp = [ _tp ( yhat , y , t ) for t in thresholds ] auc = np . trapz ( prec , x = tp ) plt . plot ( tp , prec , label = f 'AUC={auc:.3f}' ) plt . xlabel ( 'Recall' ) plt . ylabel ( 'Precision' ) plt . legend () _plot_to ( f '{path}/prc_{suffix}' ) return auc , _prec ( yhat , y , 0 . 5 ), _tp ( yhat , y , 0 . 5 ) prediction_hist def prediction_hist ( yhat , y , path ) plot the prediction scores, assuming two classes View Source def prediction_hist ( yhat , y , path ): '''plot the prediction scores, assuming two classes''' bins = np . linspace ( 0 , 1 , 25 ) plt . hist ( yhat [ y == 0 ], bins = bins , alpha = 0 . 5 , label = 'Outcome 0' ) plt . hist ( yhat [ y == 1 ], bins = bins , alpha = 0 . 5 , label = 'Outcome 1' ) plt . legend () _plot_to ( f '{path}/yhat' ) roc def roc ( yhat , y , path , suffix ) compute ROC curve and return AUC Parameters: yhat (ndarray): prediction score for the positive class y (ndarray): binary label path (str): output directory to which the plot should be saved suffix (str): label to make the plot filename unique Returns: float: area under the ROC curve View Source def roc ( yhat , y , path , suffix ): ''' compute ROC curve and return AUC Parameters: - yhat (ndarray): prediction score for the positive class - y (ndarray): binary label - path (str): output directory to which the plot should be saved - suffix (str): label to make the plot filename unique Returns: - float: area under the ROC curve ''' thresholds = np . linspace ( 1 , 0 , 25 ) fp = [ _fp ( yhat , y , t ) for t in thresholds ] tp = [ _tp ( yhat , y , t ) for t in thresholds ] auc = np . trapz ( tp , x = fp ) plt . plot ( fp , tp , label = f 'AUC={auc:.3f}' ) plt . xlabel ( 'False Positive Rate' ) plt . ylabel ( 'True Positive Rate' ) plt . legend () _plot_to ( f '{path}/roc_{suffix}' ) return auc","title":"Plot"},{"location":"reference/braf/plot/#module-brafplot","text":"View Source import numpy as np import matplotlib.pyplot as plt EPS = 1e-12 def _plot_to ( path ): '''save a plot''' plt . tight_layout () # print(f'Saving to {path}') for ext in ( 'pdf' , 'png' ): plt . savefig ( f ' { path } . { ext } ' ) plt . clf () def df_hist ( df , path , suffix ): '''plot all columns as a dataframe as histograms''' df . hist ( bins = 100 ) _plot_to ( f ' { path } /features_ { suffix } ' ) def prediction_hist ( yhat , y , path ): '''plot the prediction scores, assuming two classes''' bins = np . linspace ( 0 , 1 , 25 ) plt . hist ( yhat [ y == 0 ], bins = bins , alpha = 0.5 , label = 'Outcome 0' ) plt . hist ( yhat [ y == 1 ], bins = bins , alpha = 0.5 , label = 'Outcome 1' ) plt . legend () _plot_to ( f ' { path } /yhat' ) def _fp ( yhat , y , c ): '''compute false positives with a decision threshold c''' mask = yhat > c fps = np . logical_and ( mask , y == 0 ) . sum () total = ( y == 0 ) . sum () + EPS return fps / total def _tp ( yhat , y , c ): '''compute true positives with a decision threshold c''' mask = yhat > c tps = np . logical_and ( mask , y == 1 ) . sum () total = ( y == 1 ) . sum () + EPS return tps / total def roc ( yhat , y , path , suffix ): ''' compute ROC curve and return AUC Parameters: - yhat (ndarray): prediction score for the positive class - y (ndarray): binary label - path (str): output directory to which the plot should be saved - suffix (str): label to make the plot filename unique Returns: - float: area under the ROC curve ''' thresholds = np . linspace ( 1 , 0 , 25 ) fp = [ _fp ( yhat , y , t ) for t in thresholds ] tp = [ _tp ( yhat , y , t ) for t in thresholds ] auc = np . trapz ( tp , x = fp ) plt . plot ( fp , tp , label = f 'AUC= { auc : .3f } ' ) plt . xlabel ( 'False Positive Rate' ) plt . ylabel ( 'True Positive Rate' ) plt . legend () _plot_to ( f ' { path } /roc_ { suffix } ' ) return auc def _prec ( yhat , y , c ): '''compute precision with a decision threshold c''' mask = yhat > c tps = np . logical_and ( mask , y == 1 ) . sum () total = mask . sum () + EPS return tps / total def prc ( yhat , y , path , suffix ): ''' compute PRC curve and return AUC Parameters: - yhat (ndarray): prediction score for the positive class - y (ndarray): binary label - path (str): output directory to which the plot should be saved - suffix (str): label to make the plot filename unique Returns: - float: area under the PRC curve ''' thresholds = np . linspace ( 1 , 0 , 25 ) prec = [ _prec ( yhat , y , t ) for t in thresholds ] tp = [ _tp ( yhat , y , t ) for t in thresholds ] auc = np . trapz ( prec , x = tp ) plt . plot ( tp , prec , label = f 'AUC= { auc : .3f } ' ) plt . xlabel ( 'Recall' ) plt . ylabel ( 'Precision' ) plt . legend () _plot_to ( f ' { path } /prc_ { suffix } ' ) return auc , _prec ( yhat , y , 0.5 ), _tp ( yhat , y , 0.5 )","title":"Module braf.plot"},{"location":"reference/braf/plot/#variables","text":"EPS","title":"Variables"},{"location":"reference/braf/plot/#functions","text":"","title":"Functions"},{"location":"reference/braf/plot/#df_hist","text":"def df_hist ( df , path , suffix ) plot all columns as a dataframe as histograms View Source def df_hist ( df , path , suffix ): '''plot all columns as a dataframe as histograms''' df . hist ( bins = 100 ) _plot_to ( f '{path}/features_{suffix}' )","title":"df_hist"},{"location":"reference/braf/plot/#prc","text":"def prc ( yhat , y , path , suffix ) compute PRC curve and return AUC Parameters: yhat (ndarray): prediction score for the positive class y (ndarray): binary label path (str): output directory to which the plot should be saved suffix (str): label to make the plot filename unique Returns: float: area under the PRC curve View Source def prc ( yhat , y , path , suffix ): ''' compute PRC curve and return AUC Parameters: - yhat (ndarray): prediction score for the positive class - y (ndarray): binary label - path (str): output directory to which the plot should be saved - suffix (str): label to make the plot filename unique Returns: - float: area under the PRC curve ''' thresholds = np . linspace ( 1 , 0 , 25 ) prec = [ _prec ( yhat , y , t ) for t in thresholds ] tp = [ _tp ( yhat , y , t ) for t in thresholds ] auc = np . trapz ( prec , x = tp ) plt . plot ( tp , prec , label = f 'AUC={auc:.3f}' ) plt . xlabel ( 'Recall' ) plt . ylabel ( 'Precision' ) plt . legend () _plot_to ( f '{path}/prc_{suffix}' ) return auc , _prec ( yhat , y , 0 . 5 ), _tp ( yhat , y , 0 . 5 )","title":"prc"},{"location":"reference/braf/plot/#prediction_hist","text":"def prediction_hist ( yhat , y , path ) plot the prediction scores, assuming two classes View Source def prediction_hist ( yhat , y , path ): '''plot the prediction scores, assuming two classes''' bins = np . linspace ( 0 , 1 , 25 ) plt . hist ( yhat [ y == 0 ], bins = bins , alpha = 0 . 5 , label = 'Outcome 0' ) plt . hist ( yhat [ y == 1 ], bins = bins , alpha = 0 . 5 , label = 'Outcome 1' ) plt . legend () _plot_to ( f '{path}/yhat' )","title":"prediction_hist"},{"location":"reference/braf/plot/#roc","text":"def roc ( yhat , y , path , suffix ) compute ROC curve and return AUC Parameters: yhat (ndarray): prediction score for the positive class y (ndarray): binary label path (str): output directory to which the plot should be saved suffix (str): label to make the plot filename unique Returns: float: area under the ROC curve View Source def roc ( yhat , y , path , suffix ): ''' compute ROC curve and return AUC Parameters: - yhat (ndarray): prediction score for the positive class - y (ndarray): binary label - path (str): output directory to which the plot should be saved - suffix (str): label to make the plot filename unique Returns: - float: area under the ROC curve ''' thresholds = np . linspace ( 1 , 0 , 25 ) fp = [ _fp ( yhat , y , t ) for t in thresholds ] tp = [ _tp ( yhat , y , t ) for t in thresholds ] auc = np . trapz ( tp , x = fp ) plt . plot ( fp , tp , label = f 'AUC={auc:.3f}' ) plt . xlabel ( 'False Positive Rate' ) plt . ylabel ( 'True Positive Rate' ) plt . legend () _plot_to ( f '{path}/roc_{suffix}' ) return auc","title":"roc"},{"location":"reference/braf/utils/","text":"Module braf.utils View Source import pandas as pd def mean_impute ( df , missing_vals , imputed_vals = {}): ''' impute missing values in-place Parameters: - df (Dataframe): dataframe - missing_vals (dict): a dictionary mapping col_name -> value where value indicates the feature is missing - imputed_vals (dict): an optional dictionary mapping col_name -> mean_value where mean_value is the mean of non-missing values. if not provided, it will be computed from df. Returns: - dict: a completed copy of imputed_vals ''' for col , missing_val in missing_vals . items (): mask = df [ col ] == missing_val if col not in imputed_vals : imputed_vals [ col ] = df . loc [ ~ mask , col ] . mean () df . loc [ mask , col ] = imputed_vals [ col ] return imputed_vals def standardize_features ( df , means = {}, stds = {}, skip = { 'Outcome' }): ''' standardize features in-place Parameters: - df (Dataframe): dataframe - means (dict): a dictionary mapping col_name -> mean value. if not provided, it will be computed from df. - stds (dict): a dictionary mapping col_name -> std value. if not provided, it will be computed from df. - skip (set): a set of columns to skip, e.g. label columns Returns: - dict: a completed copy of means - dict: a completed copy of stds ''' for col in df . columns : if col in skip : continue if col not in means : means [ col ], stds [ col ] = df [ col ] . mean (), df [ col ] . std () df [ col ] = ( df [ col ] - means [ col ]) / stds [ col ] return means , stds Functions mean_impute def mean_impute ( df , missing_vals , imputed_vals = {} ) impute missing values in-place Parameters: df (Dataframe): dataframe missing_vals (dict): a dictionary mapping col_name -> value where value indicates the feature is missing imputed_vals (dict): an optional dictionary mapping col_name -> mean_value where mean_value is the mean of non-missing values. if not provided, it will be computed from df. Returns: dict: a completed copy of imputed_vals View Source def mean_impute ( df , missing_vals , imputed_vals = {} ) : ''' impute missing values in-place Parameters: - df (Dataframe): dataframe - missing_vals (dict): a dictionary mapping col_name -> value where value indicates the feature is missing - imputed_vals (dict): an optional dictionary mapping col_name -> mean_value where mean_value is the mean of non-missing values. if not provided, it will be computed from df. Returns: - dict: a completed copy of imputed_vals ''' for col , missing_val in missing_vals . items () : mask = df [ col ] == missing_val if col not in imputed_vals : imputed_vals [ col ] = df . loc [ ~mask, col ] . mean () df . loc [ mask, col ] = imputed_vals [ col ] return imputed_vals standardize_features def standardize_features ( df , means = {}, stds = {}, skip = { 'Outcome' } ) standardize features in-place Parameters: df (Dataframe): dataframe means (dict): a dictionary mapping col_name -> mean value. if not provided, it will be computed from df. stds (dict): a dictionary mapping col_name -> std value. if not provided, it will be computed from df. skip (set): a set of columns to skip, e.g. label columns Returns: dict: a completed copy of means dict: a completed copy of stds View Source def standardize_features ( df , means = {} , stds = {} , skip = { 'Outcome' } ) : ''' standardize features in-place Parameters: - df (Dataframe): dataframe - means (dict): a dictionary mapping col_name -> mean value. if not provided, it will be computed from df. - stds (dict): a dictionary mapping col_name -> std value. if not provided, it will be computed from df. - skip (set): a set of columns to skip, e.g. label columns Returns: - dict: a completed copy of means - dict: a completed copy of stds ''' for col in df . columns : if col in skip : continue if col not in means : means [ col ] , stds [ col ] = df [ col ] . mean (), df [ col ] . std () df [ col ] = ( df [ col ] - means [ col ] ) / stds [ col ] return means , stds","title":"Utils"},{"location":"reference/braf/utils/#module-brafutils","text":"View Source import pandas as pd def mean_impute ( df , missing_vals , imputed_vals = {}): ''' impute missing values in-place Parameters: - df (Dataframe): dataframe - missing_vals (dict): a dictionary mapping col_name -> value where value indicates the feature is missing - imputed_vals (dict): an optional dictionary mapping col_name -> mean_value where mean_value is the mean of non-missing values. if not provided, it will be computed from df. Returns: - dict: a completed copy of imputed_vals ''' for col , missing_val in missing_vals . items (): mask = df [ col ] == missing_val if col not in imputed_vals : imputed_vals [ col ] = df . loc [ ~ mask , col ] . mean () df . loc [ mask , col ] = imputed_vals [ col ] return imputed_vals def standardize_features ( df , means = {}, stds = {}, skip = { 'Outcome' }): ''' standardize features in-place Parameters: - df (Dataframe): dataframe - means (dict): a dictionary mapping col_name -> mean value. if not provided, it will be computed from df. - stds (dict): a dictionary mapping col_name -> std value. if not provided, it will be computed from df. - skip (set): a set of columns to skip, e.g. label columns Returns: - dict: a completed copy of means - dict: a completed copy of stds ''' for col in df . columns : if col in skip : continue if col not in means : means [ col ], stds [ col ] = df [ col ] . mean (), df [ col ] . std () df [ col ] = ( df [ col ] - means [ col ]) / stds [ col ] return means , stds","title":"Module braf.utils"},{"location":"reference/braf/utils/#functions","text":"","title":"Functions"},{"location":"reference/braf/utils/#mean_impute","text":"def mean_impute ( df , missing_vals , imputed_vals = {} ) impute missing values in-place Parameters: df (Dataframe): dataframe missing_vals (dict): a dictionary mapping col_name -> value where value indicates the feature is missing imputed_vals (dict): an optional dictionary mapping col_name -> mean_value where mean_value is the mean of non-missing values. if not provided, it will be computed from df. Returns: dict: a completed copy of imputed_vals View Source def mean_impute ( df , missing_vals , imputed_vals = {} ) : ''' impute missing values in-place Parameters: - df (Dataframe): dataframe - missing_vals (dict): a dictionary mapping col_name -> value where value indicates the feature is missing - imputed_vals (dict): an optional dictionary mapping col_name -> mean_value where mean_value is the mean of non-missing values. if not provided, it will be computed from df. Returns: - dict: a completed copy of imputed_vals ''' for col , missing_val in missing_vals . items () : mask = df [ col ] == missing_val if col not in imputed_vals : imputed_vals [ col ] = df . loc [ ~mask, col ] . mean () df . loc [ mask, col ] = imputed_vals [ col ] return imputed_vals","title":"mean_impute"},{"location":"reference/braf/utils/#standardize_features","text":"def standardize_features ( df , means = {}, stds = {}, skip = { 'Outcome' } ) standardize features in-place Parameters: df (Dataframe): dataframe means (dict): a dictionary mapping col_name -> mean value. if not provided, it will be computed from df. stds (dict): a dictionary mapping col_name -> std value. if not provided, it will be computed from df. skip (set): a set of columns to skip, e.g. label columns Returns: dict: a completed copy of means dict: a completed copy of stds View Source def standardize_features ( df , means = {} , stds = {} , skip = { 'Outcome' } ) : ''' standardize features in-place Parameters: - df (Dataframe): dataframe - means (dict): a dictionary mapping col_name -> mean value. if not provided, it will be computed from df. - stds (dict): a dictionary mapping col_name -> std value. if not provided, it will be computed from df. - skip (set): a set of columns to skip, e.g. label columns Returns: - dict: a completed copy of means - dict: a completed copy of stds ''' for col in df . columns : if col in skip : continue if col not in means : means [ col ] , stds [ col ] = df [ col ] . mean (), df [ col ] . std () df [ col ] = ( df [ col ] - means [ col ] ) / stds [ col ] return means , stds","title":"standardize_features"}]}